{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApQhnhJkeEno",
        "outputId": "af0ff3a2-7325-4c37-8e93-f3222d109a81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=63f82f40ca33499f1359b5fa01de4e2eff5b13c2a12c517c0020b653363e0cb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ppSzE1E7eCP3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"NewsDataAnalysis\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_rdd = spark.sparkContext.textFile(\"news.txt\")\n",
        "\n",
        "total_news = news_rdd.count()\n",
        "\n",
        "words_rdd = news_rdd.flatMap(lambda line: line.split())\n",
        "total_words = words_rdd.count()\n",
        "first_ten_words = words_rdd.take(10)\n",
        "\n",
        "print(\"Total news items:\", total_news)\n",
        "print(\"Total words:\", total_words)\n",
        "print(\"First ten words:\", first_ten_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxT2o6kSeat_",
        "outputId": "a0d46f79-3670-4c1e-88ae-d18316034a47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total news items: 12\n",
            "Total words: 2787\n",
            "First ten words: ['JAPAN', 'TO', 'REVISE', 'LONG', '-', 'TERM', 'ENERGY', 'DEMAND', 'DOWNWARDS', 'The']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lower_words_rdd = words_rdd.map(lambda word: word.lower())\n",
        "word_counts = lower_words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "sorted_word_counts = word_counts.sortBy(lambda word_count: word_count[1], ascending=False)\n",
        "top_ten_words = sorted_word_counts.take(10)\n",
        "print(\"Top ten words:\", top_ten_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW33IEcreoNJ",
        "outputId": "7bb160d9-94cb-43fc-cb1b-c4a608d02299"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top ten words: [('.', 130), ('the', 123), (',', 102), ('to', 84), ('of', 64), ('said', 55), ('and', 55), ('in', 54), ('a', 45), ('s', 33)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_alpha(word):\n",
        "    return word.isalpha()\n",
        "filtered_words = sorted_word_counts.filter(lambda word_count: is_alpha(word_count[0]))\n",
        "top_ten_filtered_words = filtered_words.take(10)\n",
        "print(\"Top ten filtered words:\", top_ten_filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEx0o7O_euTv",
        "outputId": "e71dcadf-54ee-4615-8a5c-d13fd3f99cbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top ten filtered words: [('the', 123), ('to', 84), ('of', 64), ('said', 55), ('and', 55), ('in', 54), ('a', 45), ('s', 33), ('on', 28), ('for', 22)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_letter_counts = filtered_words.map(lambda word_count: (word_count[0][0], 1)).reduceByKey(lambda a, b: a + b)\n",
        "top_five_letters = first_letter_counts.sortBy(lambda letter_count: letter_count[1], ascending=False).take(5)\n",
        "print(\"Top five letters:\", top_five_letters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmzNX9ffexuX",
        "outputId": "b24f48bd-0164-4c75-f3c9-c871c3785ec8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top five letters: [('c', 76), ('s', 74), ('p', 68), ('a', 57), ('r', 54)]\n"
          ]
        }
      ]
    }
  ]
}